{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms.functional as TF\n",
    "import random\n",
    "\n",
    "def my_segmentation_transforms(image,segmentation,new_img_h, new_img_w) :\n",
    "    \n",
    "    prob = random.random()\n",
    "    if prob >= 0.5 :\n",
    "        \n",
    "        image = TF.to_pil_image(image, mode=None)\n",
    "        segmentation = TF.to_pil_image(segmentation, mode=None)\n",
    "\n",
    "        # VERTICAL FLIP\n",
    "        image = TF.vflip(image)\n",
    "        segmentation = TF.vflip(segmentation)\n",
    "\n",
    "        # HORIZONTAL FLIP\n",
    "        image = TF.hflip(image)\n",
    "        segmentation = TF.hflip(segmentation)\n",
    "    \n",
    "    image = np.asarray(image) #convert from PIL image to a numpy array\n",
    "    segmentation = np.asarray(segmentation)\n",
    "    \n",
    "    # crop out a random 256X256 patch from the entire image\n",
    "    \n",
    "    start_x = np.random.randint(low=0, high=(new_img_h - 384))\n",
    "    start_y = np.random.randint(low=0, high=(new_img_w - 384))\n",
    "    \n",
    "    image = image[start_x:start_x+384, start_y:start_y+384, :] # (shape: (256, 256, 3))\n",
    "    segmentation = segmentation[start_x:start_x+384, start_y:start_y+384] # (shape: (256, 256))\n",
    "\n",
    "    return image, segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data\n",
    "from torchvision import transforms, datasets, models\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train_dirs = [\"jena/\", \"zurich/\", \"weimar/\", \"ulm/\", \"tubingen/\", \"stuttgart/\",\n",
    "              \"strasbourg/\", \"monchengladbach/\", \"krefeld/\", \"hanover/\",\n",
    "              \"hamburg/\", \"erfurt/\", \"dusseldorf/\", \"darmstadt/\", \"cologne/\",\n",
    "              \"bremen/\", \"bochum/\", \"aachen/\"]\n",
    "val_dirs = [\"frankfurt/\", \"munster/\", \"lindau/\"]\n",
    "#test_dirs = [\"berlin\", \"bielefeld\", \"bonn\", \"leverkusen\", \"mainz\", \"munich\"]\n",
    "\n",
    "#train_dirs = [\"aachen/\"]\n",
    "#cityscapes_data_path =\"/home/kaustavb/cityscapes/leftImg8bit_trainvaltest_small\"\n",
    "#cityscapes_meta_path = \"/home/kaustavb/cityscapes/gtFine_trainvaltest_small\"\n",
    "\n",
    "class DatasetTrain(torch.utils.data.Dataset):\n",
    "    def __init__(self, set1, cityscapes_data_path, cityscapes_meta_path, transform=None):\n",
    "        self.img_dir = cityscapes_data_path + \"/leftImg8bit/\"+set1+\"/\"\n",
    "        self.label_dir = cityscapes_meta_path + \"/label_imgs/\"+set1+\"/\"\n",
    "\n",
    "        self.img_h = 1024\n",
    "        self.img_w = 2048\n",
    "\n",
    "        self.new_img_h = 512\n",
    "        self.new_img_w = 1024\n",
    "\n",
    "        self.examples = []\n",
    "        \n",
    "        #select the appropriate directory\n",
    "        \n",
    "        if set1 == \"train\":\n",
    "            dirs = train_dirs\n",
    "            n_append = 1 # each pair of (image,label) is added 5 times to the tuple of train dataset \n",
    "        else :\n",
    "            dirs = val_dirs\n",
    "            n_append = 1\n",
    "            \n",
    "        for _dir in dirs:\n",
    "            img_dir_path = self.img_dir + _dir\n",
    "\n",
    "            file_names = os.listdir(img_dir_path)\n",
    "            for file_name in file_names:\n",
    "                img_id = file_name.split(\"_leftImg8bit.png\")[0]\n",
    "\n",
    "                img_path = img_dir_path + file_name\n",
    "                #label_img_path = self.label_dir + _dir + img_id + \"_gtFine_labelIds.png\" #----->\n",
    "                label_img_path = self.label_dir + _dir + img_id + \".png\"\n",
    "\n",
    "                example = {}\n",
    "                example[\"img_path\"] = img_path\n",
    "                example[\"label_img_path\"] = label_img_path\n",
    "                example[\"img_id\"] = img_id\n",
    "\n",
    "                for j in range(n_append): # each pair of (image,label) is added 5 times to the tuple of train dataset \n",
    "                    self.examples.append(example)\n",
    "\n",
    "        self.num_examples = len(self.examples)\n",
    "        self.transform = transform #add the transformation\n",
    "        \n",
    "        # the start and end indices are selected here so that a single value is selected for the cropping\n",
    "        #self.start_x = np.random.randint(low=0, high=(self.new_img_h - 256))\n",
    "        #self.start_y = np.random.randint(low=0, high=(self.new_img_w - 256))\n",
    "        #self.prob = random.random()\n",
    "        \n",
    "        #the corresponding label_IDs for the label\n",
    "        self.trainID = [19,19,19,19,19,19,19,0,1,19,19,2,3,4,19,19,19,5,19,6,7,8,9,10,11,12,13,14,15,19,19,16,17,18]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        example = self.examples[index]\n",
    "\n",
    "        img_path = example[\"img_path\"]\n",
    "        img = cv2.imread(img_path, -1) # (shape: (1024, 2048, 3))\n",
    "        # resize img without interpolation (want the image to still match\n",
    "        # label_img, which we resize below):\n",
    "        img = cv2.resize(img, (self.new_img_w, self.new_img_h),\n",
    "                         interpolation=cv2.INTER_NEAREST) # (shape: (512, 1024, 3))\n",
    "\n",
    "        label_img_path = example[\"label_img_path\"]\n",
    "        label_img = cv2.imread(label_img_path, -1) # (shape: (1024, 2048))\n",
    "        # resize label_img without interpolation (want the resulting image to\n",
    "        # still only contain pixel values corresponding to an object class):\n",
    "        label_img = cv2.resize(label_img, (self.new_img_w, self.new_img_h),\n",
    "                               interpolation=cv2.INTER_NEAREST) # (shape: (512, 1024))\n",
    "        \n",
    "        img, label_img = my_segmentation_transforms(img, label_img, self.new_img_h, self.new_img_w) #helps us get the transforms into a function\n",
    "        #img = np.asarray(img) #these steps convert to PIL image to an array.\n",
    "        #label_img = np.asarray(label_img) #these steps convert to PIL image to an array.\n",
    "        \n",
    "        #print(img.shape)\n",
    "        #print(label_img.shape)\n",
    "        #'''    \n",
    "        \n",
    "        #img = img[self.start_x:self.start_x+256, self.start_y:self.start_y+256, :] # (shape: (256, 256, 3))\n",
    "        #label_img = label_img[self.start_x:self.start_x+256, self.start_y:self.start_y+256] # (shape: (256, 256))\n",
    "        \n",
    "        ########################################################################\n",
    "        #'''\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            img = ((img.astype('float'))/255.0)\n",
    "            #label_img = ((label_img.astype('float'))/255.0) \n",
    "            #the image has labels for all the pixels. Labels correspond to the class and there are 33 classes.\n",
    "            img = self.transform(img)\n",
    "            label_img = torch.from_numpy(label_img.astype(np.int64))\n",
    "            \n",
    "            #label_img_id = 0 #pre-processing the labelled image (to 20 classes !!)\n",
    "            #for j in range(34):\n",
    "            #    label_img_id += self.trainID[j]*(label_img==j)\n",
    "            \n",
    "        return (img, label_img)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cityscapes_data_path =\"/home/kaustavb/cityscapes/leftImg8bit_trainval\"\n",
    "cityscapes_meta_path = \"/home/kaustavb/cityscapes/meta\" #gtFine_trainval\n",
    "\n",
    "trans = transforms.Compose([\n",
    "    transforms.ToTensor(), # converts the data into tensors\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) # imagenet\n",
    "])\n",
    "\n",
    "train_set = DatasetTrain(\"train\", cityscapes_data_path, cityscapes_meta_path, transform = trans)\n",
    "val_set = DatasetTrain(\"val\", cityscapes_data_path, cityscapes_meta_path, transform = trans)\n",
    "#train_loader = torch.utils.data.DataLoader(dataset=train_set,\n",
    "                                           #batch_size=10, shuffle=True,\n",
    "                                           #num_workers=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "#dict_id = {}\n",
    "trainID = [19,19,19,19,19,19,19,0,1,19,19,2,3,4,19,19,19,5,19,6,7,8,9,10,11,12,13,14,15,19,19,16,17,18]\n",
    "for idx in range(1):#len(train_set)):\n",
    "    #dict_id[idx]={}\n",
    "    train_label = 0 #torch.((256,256))\n",
    "    for j in range(34):\n",
    "         #dict_id[idx][j] = (train_set[idx][1]==j)\n",
    "        train_label += trainID[j]*(train_set[idx][1]==j)\n",
    "    plt.imshow(train_label)\n",
    "    plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_datasets = {\n",
    "    'train': train_set, 'val': val_set\n",
    "}  \n",
    "dataloaders = {\n",
    "    'train': torch.utils.data.DataLoader(train_set, batch_size=8, shuffle=True, num_workers=0),\n",
    "    'val': torch.utils.data.DataLoader(val_set, batch_size=1, shuffle=False, num_workers=0)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reverse_transform(inp):\n",
    "    inp = inp.numpy().transpose((1, 2, 0))\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    #std = 1\n",
    "    #mean = 0\n",
    "    inp = std * inp + mean\n",
    "    #inp = np.clip(inp, 0, 1)\n",
    "    inp = (inp * 255).astype(np.uint8)\n",
    "\n",
    "    return inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = train_set[100] # this is a single call to __get_item__(). This ensures that the image and the label are properly matched."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(reverse_transform(x[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_vals = []\n",
    "for idx in range(len(train_set)):\n",
    "    max_vals.append(torch.max(train_set[idx][1]))\n",
    "print(max(max_vals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_vals = []\n",
    "for idx in range(len(train_set)):\n",
    "    min_vals.append(torch.min(train_set[idx][1]))\n",
    "print(min(min_vals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set[idx][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.max(train_set[idx][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val = (train_set[idx][1]==26)\n",
    "plt.imshow(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs, masks = next(iter(dataloaders['val']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataloaders['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(reverse_transform(inputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(masks[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training on a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.deeplabv3 import DeepLabV3\n",
    "net = DeepLabV3(num_classes=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder.conv1\n",
      "encoder.layer1.0.conv1\n",
      "encoder.layer1.0.conv2\n",
      "encoder.layer1.1.conv1\n",
      "encoder.layer1.1.conv2\n",
      "encoder.layer1.2.conv1\n",
      "encoder.layer1.2.conv2\n",
      "encoder.layer2.0.conv1\n",
      "encoder.layer2.0.conv2\n",
      "encoder.layer2.0.downsample.0\n",
      "encoder.layer2.1.conv1\n",
      "encoder.layer2.1.conv2\n",
      "encoder.layer2.2.conv1\n",
      "encoder.layer2.2.conv2\n",
      "encoder.layer2.3.conv1\n",
      "encoder.layer2.3.conv2\n",
      "encoder.layer3.0.conv1\n",
      "encoder.layer3.0.conv2\n",
      "encoder.layer3.0.downsample.0\n",
      "encoder.layer3.1.conv1\n",
      "encoder.layer3.1.conv2\n",
      "encoder.layer3.2.conv1\n",
      "encoder.layer3.2.conv2\n",
      "encoder.layer3.3.conv1\n",
      "encoder.layer3.3.conv2\n",
      "encoder.layer3.4.conv1\n",
      "encoder.layer3.4.conv2\n",
      "encoder.layer3.5.conv1\n",
      "encoder.layer3.5.conv2\n",
      "encoder.layer4.0.conv1\n",
      "encoder.layer4.0.conv2\n",
      "encoder.layer4.0.downsample.0\n",
      "encoder.layer4.1.conv1\n",
      "encoder.layer4.1.conv2\n",
      "encoder.layer4.2.conv1\n",
      "encoder.layer4.2.conv2\n",
      "layer5.0.conv1\n",
      "layer5.0.conv2\n",
      "layer5.0.conv3\n",
      "layer5.1.conv1\n",
      "layer5.1.conv2\n",
      "layer5.1.conv3\n",
      "layer5.2.conv1\n",
      "layer5.2.conv2\n",
      "layer5.2.conv3\n",
      "aspp.conv_1x1_1\n",
      "aspp.conv_3x3_1\n",
      "aspp.conv_3x3_2\n",
      "aspp.conv_3x3_3\n",
      "aspp.conv_1x1_2\n",
      "aspp.conv_1x1_3\n",
      "aggregate.0\n",
      "last_conv.0\n",
      "last_conv.4\n",
      "last_conv.8\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "for n,m in net.named_modules():\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary\n",
    "summary(net.cuda(), input_size = (3, 384, 384))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import logging\n",
    "import re\n",
    "import functools\n",
    "import fnmatch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.initialized = False\n",
    "        self.val = None\n",
    "        self.avg = None\n",
    "        self.sum = None\n",
    "        self.count = None\n",
    "\n",
    "    def initialize(self, val, weight):\n",
    "        self.val = val\n",
    "        self.avg = val\n",
    "        self.sum = val * weight\n",
    "        self.count = weight\n",
    "        self.initialized = True\n",
    "\n",
    "    def update(self, val, weight=1):\n",
    "        if not self.initialized:\n",
    "            self.initialize(val, weight)\n",
    "        else:\n",
    "            self.add(val, weight)\n",
    "\n",
    "    def add(self, val, weight):\n",
    "        self.val = val\n",
    "        self.sum += val * weight\n",
    "        self.count += weight\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "    def value(self):\n",
    "        return self.val\n",
    "\n",
    "    def average(self):\n",
    "        return self.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "import torch.nn as nn\n",
    "\n",
    "class SegmentationModuleBase(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SegmentationModuleBase, self).__init__()\n",
    "\n",
    "    def pixel_acc(self, pred, label, n=20): #-->n:number of classes\n",
    "        # pred : already is a probability value as a softmax layer is present as the last layer of the CNN.\n",
    "        #----------------------mIOU accuracy in a new way----------------------- (hard IOU):not using prob. of prediction\n",
    "        _, preds = torch.max(pred.data.cpu(), dim=1)\n",
    "\n",
    "        # compute area intersection\n",
    "        intersect = preds.clone()\n",
    "        segs = label.data.cpu()\n",
    "        intersect[torch.ne(preds, segs)] = -1\n",
    "\n",
    "        area_intersect = torch.histc(intersect.float(),\n",
    "                                     bins=n,\n",
    "                                     min=0,\n",
    "                                     max=n-1)\n",
    "\n",
    "        # compute area union:\n",
    "        preds[torch.lt(segs, 0)] = -1\n",
    "        area_pred = torch.histc(preds.float(),\n",
    "                                bins=n,\n",
    "                                min=0,\n",
    "                                max=n-1)\n",
    "        area_lab = torch.histc(segs.float(),\n",
    "                               bins=n,\n",
    "                               min=0,\n",
    "                               max=n-1)\n",
    "        area_union = area_pred + area_lab - area_intersect\n",
    "        mIOU = area_intersect/(area_union + 1e-10)\n",
    "        #-----------------------------------------------------------------------\n",
    "        _, preds = torch.max(pred.data.cpu(), dim=1)\n",
    "        segs = label.data.cpu()\n",
    "        \n",
    "        valid = (segs >= 0).long()\n",
    "        acc_sum = torch.sum(valid * (preds == segs).long())\n",
    "        pixel_sum = torch.sum(valid)\n",
    "        acc = acc_sum.float() / (pixel_sum.float() + 1e-10)\n",
    "        \n",
    "        #calculate individual accuracies for each image in the minibatch\n",
    "        acc_sum1 = torch.sum(valid * (preds == segs).long(), (1,2))\n",
    "        pixel_sum1 = torch.sum(valid, (1,2))\n",
    "        acc1 = acc_sum1.float() / (pixel_sum1.float() + 1e-10)\n",
    "        \n",
    "        #return acc\n",
    "        return acc.mean(), mIOU # return mean accuracy over the minibatch.\n",
    "\n",
    "\n",
    "class SegmentationModule(SegmentationModuleBase):\n",
    "    def __init__(self, net, crit, n_class=20):\n",
    "        super(SegmentationModule, self).__init__()\n",
    "        self.net = net\n",
    "        self.crit = crit\n",
    "        self.n = n_class\n",
    "\n",
    "    def forward(self, inp, label):\n",
    "        # training\n",
    "        pred = self.net(inp)\n",
    "        #print(pred.shape)\n",
    "        loss = self.crit(pred, label)\n",
    "        acc, mIOU = self.pixel_acc(pred, label,self.n)        \n",
    "        return loss, acc, mIOU\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "def adjust_learning_rate(optimizers, cur_iter, epoch_iters):\n",
    "    scale_running_lr = ((1. - float(cur_iter) / epoch_iters) ** 0.9)\n",
    "    running_lr_encoder = 0.02 * scale_running_lr #both are same values for now.\n",
    "    running_lr_decoder = 0.02 * scale_running_lr\n",
    "    \n",
    "    #(optimizer_encoder, optimizer_decoder) = optimizers\n",
    "    #for param_group in optimizer_encoder.param_groups:\n",
    "    #    param_group['lr'] = running_lr_encoder\n",
    "    #for param_group in optimizer_decoder.param_groups:\n",
    "    #    param_group['lr'] = running_lr_decoder\n",
    "    \n",
    "    for param_group in optimizers.param_groups:\n",
    "        param_group['lr'] = running_lr_decoder # running_lr_decoder = running_lr_encoder\n",
    "    return running_lr_encoder, running_lr_decoder\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def adjust_learning_rate(optimizer_e, optimizer_d, epoch, n_epoch, lr_e=0.005, lr_d=0.01, lr_type='cos'):\n",
    "    if lr_type == 'cos':  # cos without warm-up\n",
    "        lr_e = 0.5 * lr_e * (1 + math.cos(math.pi * epoch / n_epoch))\n",
    "        lr_d = 0.5 * lr_d * (1 + math.cos(math.pi * epoch / n_epoch))\n",
    "    elif lr_type == 'exp':\n",
    "        step = 1\n",
    "        decay = 0.96\n",
    "        lr_e = lr_e * (decay ** (epoch // step))\n",
    "        lr_d = lr_d * (decay ** (epoch // step))\n",
    "    elif lr_type == 'fixed':\n",
    "        lr_e = lr_e\n",
    "        lr_d = lr_d\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    print('=> lr_encoder: {}'.format(lr_e))\n",
    "    print('=> lr_decoder: {}'.format(lr_d))\n",
    "    for param_group in optimizer_e.param_groups:\n",
    "        param_group['lr'] = lr_e\n",
    "    for param_group in optimizer_d.param_groups:\n",
    "        param_group['lr'] = lr_d\n",
    "        \n",
    "    return lr_e, lr_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "def create_optimizers(net):\n",
    "    #optimizers = optim.Adam(filter(lambda p: p.requires_grad, net.parameters()), lr=0.02)\n",
    "    optimizers_enc = optim.Adam([{'params':net.encoder.parameters(), 'lr':0.02}], lr=0.02)\n",
    "    \n",
    "    optimizers_dec = optim.Adam([{'params':net.layer5.parameters(), 'lr':0.02},\n",
    "                                 {'params':net.aspp.parameters(), 'lr':0.02},\n",
    "                                 {'params':net.aggregate.parameters(), 'lr':0.02},\n",
    "                                 {'params':net.last_conv.parameters(), 'lr':0.02}], lr=0.02)\n",
    "    \n",
    "    return optimizers_enc, optimizers_dec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pixel_acc(pred, label, n=20): #-->n:number of classes\n",
    "        # pred : already is a probability value as a softmax layer is present as the last layer of the CNN.\n",
    "        #----------------------mIOU accuracy in a new way----------------------- (hard IOU):not using prob. of prediction\n",
    "        _, preds = torch.max(pred.data.cpu(), dim=1)\n",
    "\n",
    "        # compute area intersection\n",
    "        intersect = preds.clone()\n",
    "        segs = label.data.cpu()\n",
    "        intersect[torch.ne(preds, segs)] = -1\n",
    "\n",
    "        area_intersect = torch.histc(intersect.float(),\n",
    "                                     bins=n,\n",
    "                                     min=0,\n",
    "                                     max=n-1)\n",
    "\n",
    "        # compute area union:\n",
    "        preds[torch.lt(segs, 0)] = -1\n",
    "        area_pred = torch.histc(preds.float(),\n",
    "                                bins=n,\n",
    "                                min=0,\n",
    "                                max=n-1)\n",
    "        area_lab = torch.histc(segs.float(),\n",
    "                               bins=n,\n",
    "                               min=0,\n",
    "                               max=n-1)\n",
    "        area_union = area_pred + area_lab - area_intersect\n",
    "        mIOU = area_intersect/(area_union + 1e-10)\n",
    "        #-----------------------------------------------------------------------\n",
    "        _, preds = torch.max(pred.data.cpu(), dim=1)\n",
    "        segs = label.data.cpu()\n",
    "        \n",
    "        valid = (segs >= 0).long()\n",
    "        acc_sum = torch.sum(valid * (preds == segs).long())\n",
    "        pixel_sum = torch.sum(valid)\n",
    "        acc = acc_sum.float() / (pixel_sum.float() + 1e-10)\n",
    "        \n",
    "        #calculate individual accuracies for each image in the minibatch\n",
    "        acc_sum1 = torch.sum(valid * (preds == segs).long(), (1,2))\n",
    "        pixel_sum1 = torch.sum(valid, (1,2))\n",
    "        acc1 = acc_sum1.float() / (pixel_sum1.float() + 1e-10)\n",
    "        \n",
    "        #return acc\n",
    "        return acc.mean(), mIOU # return mean accuracy over the minibatch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(segmentation_module, iterator,  optimizer_e, optimizer_d, history, epoch, len_iterator, n_epochs, \n",
    "          running_lr_e, running_lr_d, crit, n_class=20):\n",
    "    #---> len_iterator : one epoch has how many mini-batches (to traverse through the dataset)\n",
    "    #---> n_epochs : total number of epochs \n",
    "\n",
    "    ave_total_loss = AverageMeter()\n",
    "    ave_acc = AverageMeter()\n",
    "    ave_miou = AverageMeter()\n",
    "    \n",
    "    #segmentation_module.net.train()\n",
    "    #segmentation_module.zero_grad()\n",
    "    segmentation_module.train()\n",
    "    segmentation_module.zero_grad()\n",
    "    \n",
    "    # main loop\n",
    "    for i in range(len_iterator):\n",
    "        # load a batch of data\n",
    "        inputs, masks = next(iterator)\n",
    "        inputs, masks = inputs.cuda(), masks.cuda()\n",
    "        optimizer_e.zero_grad()\n",
    "        optimizer_d.zero_grad()\n",
    "\n",
    "        # adjust learning rate\n",
    "        #cur_iter = i + (epoch - 1) * len_iterator\n",
    "        #running_lr_encoder, running_lr_decoder = adjust_learning_rate(optimizers, cur_iter, len_iterator*n_epochs)\n",
    "        \n",
    "\n",
    "        # forward pass\n",
    "        #loss, acc, mIOU = segmentation_module(inputs.float(),masks.long())\n",
    "        \n",
    "        pred = segmentation_module(inputs.float()) #,masks.long())\n",
    "        loss = crit(pred, masks.long())\n",
    "        acc, mIOU = pixel_acc(pred, masks.long(), n_class)\n",
    "        \n",
    "        #print(loss)\n",
    "        #print(acc)\n",
    "        loss = loss.mean()\n",
    "        acc = acc.mean()\n",
    "        mIOU = mIOU.mean()\n",
    "        \n",
    "        #print(loss)\n",
    "        #print(acc)\n",
    "        \n",
    "        # Backward\n",
    "        loss.backward()\n",
    "        #for optimizer in optimizers:\n",
    "        optimizer_e.step()\n",
    "        optimizer_d.step()\n",
    "\n",
    "        # update average loss and acc\n",
    "        ave_total_loss.update(loss.data.item())\n",
    "        ave_acc.update(acc.data.item()*100)\n",
    "        ave_miou.update(mIOU.data.item()*100)\n",
    "\n",
    "        # calculate accuracy, and display\n",
    "        if i % 100 == 0:\n",
    "            print('Epoch: [{}][{}/{}], '\n",
    "                  'lr_encoder: {:.6f}, lr_decoder: {:.6f}, '\n",
    "                  'Accuracy: {:4.2f}, Loss: {:.6f}, mIOU: {:.6f}'\n",
    "                  .format(epoch, i, len_iterator,\n",
    "                          running_lr_e, running_lr_d,\n",
    "                          ave_acc.average(), ave_total_loss.average(), ave_miou.average()))\n",
    "\n",
    "            fractional_epoch = epoch - 1 + 1. * i / len_iterator\n",
    "            history['train']['epoch'].append(fractional_epoch)\n",
    "            history['train']['loss'].append(loss.data.item())\n",
    "            history['train']['acc'].append(acc.data.item())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "def val(segmentation_module, iterator, history, len_iterator, crit, n_class):\n",
    "    global best_acc\n",
    "    #---> len_iterator : one epoch has how many mini-batches (to traverse through the dataset)\n",
    "    #---> n_epochs : total number of epochs \n",
    "\n",
    "    ave_total_loss = AverageMeter()\n",
    "    ave_acc = AverageMeter()\n",
    "    ave_miou = AverageMeter()\n",
    "    \n",
    "    #segmentation_module.net.eval()\n",
    "    #segmentation_module.zero_grad()\n",
    "    segmentation_module.eval()\n",
    "    \n",
    "    # main loop\n",
    "    for i in range(len_iterator):\n",
    "        # load a batch of data\n",
    "        inputs, masks = next(iterator)\n",
    "        inputs, masks = inputs.cuda(), masks.cuda()\n",
    "        \n",
    "        # forward pass\n",
    "        #loss, acc, mIOU = segmentation_module(inputs.float(),masks.long())\n",
    "        \n",
    "        pred = segmentation_module(inputs.float()) #,masks.long())\n",
    "        loss = crit(pred, masks.long())\n",
    "        acc, mIOU = pixel_acc(pred, masks.long(), n_class)\n",
    "\n",
    "        loss = loss.mean()\n",
    "        acc = acc.mean()\n",
    "        mIOU = mIOU.mean()\n",
    "\n",
    "        # update average loss and acc\n",
    "        ave_total_loss.update(loss.data.item())\n",
    "        ave_acc.update(acc.data.item()*100)\n",
    "        ave_miou.update(mIOU.data.item()*100)\n",
    "\n",
    "        # calculate accuracy, and display\n",
    "        if i%400 == 0: #print once over val dataset\n",
    "            print('Accuracy: {:4.2f}, Loss: {:.6f}, mIOU: {:.6f}'\n",
    "                  .format(ave_acc.average(), ave_total_loss.average(), ave_miou.average()))\n",
    "\n",
    "            history['val']['loss'].append(loss.data.item())\n",
    "            history['val']['acc'].append(acc.data.item())\n",
    "            \n",
    "        if acc > best_acc:\n",
    "            best_acc = acc           \n",
    "            best_model_wts = copy.deepcopy(segmentation_module.state_dict())\n",
    "            #torch.save(best_model_wts, \"weight/deeplabv3_resnet34_weights_fpia_qw5_abc.pth\")\n",
    "            torch.save(best_model_wts, \"weight/deeplabv3_resnet34_weights_qia5_qw5_abc.pth\") \n",
    "            #this has to be dynamically changed based on what type of network we are running at present.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from torch.autograd import Variable\n",
    "\n",
    "with open(\"/home/kaustavb/cityscapes/meta/class_weights.pkl\", \"rb\") as file: # (needed for python3)\n",
    "    class_weights = np.array(pickle.load(file))\n",
    "class_weights = torch.from_numpy(class_weights)\n",
    "class_weights = Variable(class_weights.type(torch.FloatTensor)).cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crit = nn.NLLLoss(weight=class_weights, ignore_index=-1)\n",
    "net = DeepLabV3(num_classes=7)\n",
    "net.load_state_dict(torch.load(\"/home/kaustavb/6867/weight/deeplabv3_resnet34_weights.pth\"))\n",
    "\n",
    "#segmentation_module = SegmentationModule(net, crit, n_class=7)\n",
    "#segmentation_module.cuda()\n",
    "\n",
    "net.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#iterator_train = iter(dataloaders['train'])\n",
    "history = {'train': {'epoch': [], 'loss': [], 'acc': [], 'mIOU': []}, 'val': {'loss': [], 'acc': [], 'mIOU': []}}\n",
    "\n",
    "optimizers_enc, optimizers_dec = create_optimizers(net.cuda())\n",
    "num_epoch=100\n",
    "\n",
    "best_acc = 0\n",
    "\n",
    "for epoch in range(num_epoch):\n",
    "    iterator_train = iter(dataloaders['train'])\n",
    "    running_lr_e, running_lr_d = adjust_learning_rate(optimizers_enc, optimizers_dec, epoch, num_epoch, \n",
    "                                                      lr_e=0.0125, lr_d=0.025, lr_type='cos') \n",
    "    #lr changed once per epoch \n",
    "    train(net, iterator_train, optimizers_enc, optimizers_dec, history, epoch+1, len(dataloaders['train']), \n",
    "          num_epoch, running_lr_e, running_lr_d, crit, num_classes)\n",
    "    \n",
    "    iterator_val = iter(dataloaders['val'])\n",
    "    val(net, iterator_val, history, len(dataloaders['val']), crit, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator_val = iter(dataloaders['val'])\n",
    "for i in range(len(dataloaders['val'])):\n",
    "    # load a batch of data\n",
    "    inputs, masks = next(iterator_val)\n",
    "    inputs, masks = inputs.cuda(), masks.cuda()\n",
    "    pred = net(inputs.float())\n",
    "    _, preds = torch.max(pred, dim=1)\n",
    "    plt.imshow(masks.cpu().numpy()[0])\n",
    "    plt.show()\n",
    "    plt.imshow(preds.cpu().numpy()[0])\n",
    "    plt.show()\n",
    "    print('###########################################################################################################')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Quantizing the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import time\n",
    "import argparse\n",
    "import shutil\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.autograd import Function\n",
    "import functools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def rgetattr(obj, attr, *args):\n",
    "    def _getattr(obj, attr):\n",
    "        return getattr(obj, attr, *args)\n",
    "    return functools.reduce(_getattr, [obj] + attr.split('.'))\n",
    "\n",
    "def rsetattr(obj, attr, val):\n",
    "    pre, _, post = attr.rpartition('.')\n",
    "    return setattr(rgetattr(obj, pre) if pre else obj, post, val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### The components necessary for Quantizing a network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.autograd import Function\n",
    "\n",
    "import functools\n",
    "\n",
    "\"\"\" \n",
    "class fx8(Function):    \n",
    "    @staticmethod\n",
    "    def forward(ctx, tensor): \n",
    "        Nlevels = (2**1) #assuming 8 bit quantization\n",
    "        if torch.min(tensor) >= 0 :\n",
    "            delta = torch.max(tensor)/Nlevels\n",
    "        else :\n",
    "            delta = (torch.max(tensor)-torch.min(tensor))/Nlevels\n",
    "        \n",
    "        xint = torch.round(torch.div(tensor,delta))\n",
    "        if torch.min(tensor) >= 0 :\n",
    "            xq = torch.clamp(xint, 0, Nlevels-1)\n",
    "        else :\n",
    "            xq = torch.clamp(xint, -Nlevels/2, Nlevels/2-1)\n",
    "        xfloat = torch.mul(xq,delta)         \n",
    "        return xfloat\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        # We return as many input gradients as there were arguments.\n",
    "        # Gradients of non-Tensor arguments to forward must be None.\n",
    "        grad_input = grad_output.clone()\n",
    "        return grad_input #, None\n",
    "        \n",
    "\"\"\" \n",
    "#-----------------------> (added on 31-07-2020)\n",
    "class AQuantizer(Function):    \n",
    "    @staticmethod\n",
    "    def forward(ctx, tensor, shift_v, N):\n",
    "        #tensor1 = torch.unsqueeze(tensor,0).repeat(N,1,1,1,1)\n",
    "        tensor1 = torch.cat(N*[torch.unsqueeze(tensor,0)]) #--> same as the above but the above giving issue with backward pass.\n",
    "        shift_v = shift_v.unsqueeze(1).unsqueeze(1).unsqueeze(1).unsqueeze(1) # shape : NX1X1X1X1\n",
    "        '''\n",
    "        x = torch.clamp(tensor1+shift_v, 0, 1) \n",
    "        ctx.save_for_backward(tensor1, shift_v)\n",
    "        x[x>=0.5] = 1.0\n",
    "        x[x<0.5] = 0.0\n",
    "        x = 2.0*x-1.0\n",
    "        return x\n",
    "        '''\n",
    "        x = tensor1-shift_v\n",
    "        ctx.save_for_backward(x)\n",
    "        y = torch.sign(x)\n",
    "        return y\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):        \n",
    "        # We return as many input gradients as there were arguments.\n",
    "        # Gradients of non-Tensor arguments to forward must be None.\n",
    "        \n",
    "        g_o = grad_output.clone() #this is a 5D tensor : Nxbatch-sizexchannelxRxC\n",
    "        #print(ctx.saved_tensors)\n",
    "        t = ctx.saved_tensors[0] #--> otherwise reyurning a tuple\n",
    "        '''\n",
    "        t,v = ctx.saved_tensors\n",
    "        \n",
    "        #t = t+v #-----> this is commented out for best performance in mobilenet as Encoder case.\n",
    "        t[t==0.0]=1.0\n",
    "        t[t>1.0]=0.0\n",
    "        t[t<0.0]=0.0  \n",
    "        t[t>0.0]=1.0\n",
    "        grad_input=g_o*t\n",
    "        grad_input = (1/g_o.size()[0])*torch.sum(grad_input, dim=0)\n",
    "        return grad_input , None, None\n",
    "        '''\n",
    "        \n",
    "        #t = t-v\n",
    "        #t[t==0.0] = 1.0\n",
    "        t1 = t.clone()          #--->new\n",
    "        t1 = -2*torch.abs(t1)+2 #--->new\n",
    "        \n",
    "        t[torch.abs(t)<=1.0] = 1.0\n",
    "        t[torch.abs(t)>1.0] = 0.0 \n",
    "        \n",
    "        t = t1*t                #--->new\n",
    "        \n",
    "        grad_input=grad_output*t\n",
    "        grad_input=(1/grad_output.size()[0])*torch.sum(grad_input, dim=0)\n",
    "        \n",
    "        #average the gradient along the batch-size dimension\n",
    "        grad_a = (1/t.size()[1])*(torch.sum(torch.sum(torch.sum(torch.sum(g_o,dim=1),dim=1),dim=1),dim=1))*-1.0\n",
    "        #N-element tensor returned.\n",
    "        return grad_input , grad_a, None\n",
    "        \n",
    "'''\n",
    "class ActIQuantizer(nn.Module) : #activation Identity quantizer\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(ActIQuantizer, self).__init__()\n",
    "        self.shift_init = kwargs['shift_init']\n",
    "        #self.shift_v = nn.Parameter(torch.from_numpy(np.array(self.shift_init)).float()) #initial clip_v value\n",
    "        self.register_buffer('shift_v', torch.from_numpy(np.array(self.shift_init)).float())\n",
    "   \n",
    "    def forward(self, input):\n",
    "\n",
    "        return input\n",
    "'''\n",
    "\n",
    "#-----------------------> (added on 27=07-2020)\n",
    "class ActQuantizer(nn.Module) :\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(ActQuantizer, self).__init__()\n",
    "        self.shift_init = kwargs['shift_init']\n",
    "        self.N = kwargs['N']\n",
    "        #self.shift_v = nn.Parameter(torch.from_numpy(np.array(self.shift_init)).float()) #initial clip_v value\n",
    "        self.shift_v = nn.Parameter(torch.randn(self.N,).float()) #initial clip_v value\n",
    "        #self.register_buffer('shift_v', torch.from_numpy(np.array(self.shift_init)).float())        \n",
    "        \n",
    "        #self.register_backward_hook(self.backward_hook) #---> This is not called when backward_hook() is not called.\n",
    "    \n",
    "    def forward(self, input):\n",
    "        x = AQuantizer.apply(input,self.shift_v,self.N)#-->new addition\n",
    "        return x\n",
    "    \n",
    "class WQuantizer(nn.Module):\n",
    "    \n",
    "    def __init__(self, *kargs, **kwargs):\n",
    "        super(WQuantizer, self).__init__()\n",
    "        self.M = kwargs['M']\n",
    "        self.register_buffer('u', torch.tensor(np.zeros( (self.M,1,1,1,1) ) ) )\n",
    "        for i in range(self.M):\n",
    "            self.u[i,0,0,0,0] = -1+2*(i-1)/(self.M-1)        \n",
    "        data = kwargs['data']\n",
    "        \n",
    "    def quantize(self, data):        \n",
    "        data = torch.unsqueeze(data,0) \n",
    "        B_concat = torch.sign(data-torch.mean(data) + self.u*torch.std(data)).float() #-->new (all Bi's along 0 th dimension)       \n",
    "        #calculate 'a'        \n",
    "        # the .float() was added to ensure all operations in float() mode. Otherwise, it was giving error saying input is float and weight double()        \n",
    "        W1 = torch.reshape(data,(-1,1))           #-->added .float()\n",
    "        B1 = torch.reshape(B_concat,(self.M,-1)) #-->new\n",
    "        B = torch.transpose(B1,0,1)              #-->new\n",
    "        a = torch.matmul(torch.matmul(torch.pinverse(torch.matmul(torch.transpose(B,0,1),B)),torch.transpose(B,0,1)),W1).float()\n",
    "        \n",
    "        return a,B_concat\n",
    "\n",
    "class QConv2d(nn.Conv2d):\n",
    "    \n",
    "    def __init__(self, quant_args=None, init_args=None, *kargs, **kwargs):\n",
    "        super(QConv2d, self).__init__(*kargs, **kwargs)\n",
    "        # ....................................................weight quantization\n",
    "        self.weight.data = init_args['weight_data']\n",
    "        if kwargs['bias'] == True:\n",
    "            self.bias.data = init_args['bias_data']\n",
    "        self.M = init_args['M']\n",
    "        w_qargs = {'M':self.M}\n",
    "        self.quantizer = WQuantizer (data = self.weight.data, **w_qargs)\n",
    "        \n",
    "        a_copy = np.zeros((self.M,1)) #--> new\n",
    "        a_copy[0][0]=1.0 #--> new\n",
    "        self.register_buffer('a', torch.tensor(a_copy)) #--> new\n",
    "        #self.register_buffer('a', torch.tensor([[1],[0],[0]])) \n",
    "        \n",
    "        qB_copy = torch.unsqueeze(self.weight.clone(),0) #--> new\n",
    "        qB1_copy = qB_copy #--> new\n",
    "        for i in range(self.M-1) : #--> new\n",
    "            qB_copy = torch.cat((qB_copy,qB1_copy),0) #--> new\n",
    "        self.qB = nn.Parameter(qB_copy) #--> new\n",
    "        \n",
    "        \n",
    "        # .....................................................input quantization \n",
    "        self.N = init_args['N']\n",
    "        \n",
    "        if self.N == 3 : #input quantization present.\n",
    "            i_qargs = {'shift_init': [-0.8,0,0.8],'N': self.N}\n",
    "            self.input_quantizer = ActQuantizer(**i_qargs)            \n",
    "            #self.register_buffer('b', torch.tensor([[1.0], [1.0], [1.0]])) #--> try as non-trainable parameter once.\n",
    "            self.b = [[1.0], [1.0], [1.0]]\n",
    "        \n",
    "        elif self.N == 5 : #input quantization present.\n",
    "            i_qargs = {'shift_init': [-0.8,-0.3,0,0.3,0.8],'N': self.N}\n",
    "            #i_qargs = {'shift_init': [-1.5,-0.5,0,0.5,1.5],'N': self.N}\n",
    "            self.input_quantizer = ActQuantizer(**i_qargs)            \n",
    "            #self.register_buffer('b', torch.tensor([[1.0], [1.0], [1.0]])) #--> try as non-trainable parameter once.\n",
    "            self.b = [[1.0], [1.0], [1.0], [1.0], [1.0]]\n",
    "        \n",
    "        \n",
    "    #call it after loss.backward()\n",
    "    #---> specifically added for DeepLabv3+ (as last layer of ResNet-18 is not used by the code, so no gradient propagation from there)#KB(added on 01-08-2020)\n",
    "    def update_grads(self):\n",
    "        if self.qB.grad is not None :    \n",
    "            w_grad = 0.0\n",
    "            for i in range(self.M):\n",
    "                w_grad  += self.a[i][0]*self.qB.grad[i]\n",
    "            self.weight.grad = w_grad\n",
    "\n",
    "    def forward(self, input):       \n",
    " # ----------------------------------------------------------------------------N=3(number of bases for input activations.)  \n",
    "        if self.N == 0 :    \n",
    "            self.a.data, self.qB.data = self.quantizer.quantize(self.weight)\n",
    "            out = 0.0;\n",
    "            for i in range(self.M):\n",
    "                out  += self.a[i][0]*F.conv2d(input, self.qB[i], self.bias, self.stride, self.padding, self.dilation, self.groups)\n",
    "                \n",
    "        elif self.N == 3 :\n",
    "            \n",
    "            x = self.input_quantizer(input) # --> Input quantization\n",
    "            self.a.data, self.qB.data = self.quantizer.quantize(self.weight)\n",
    "\n",
    "            out1 = 0.0;\n",
    "            for i in range(self.M):\n",
    "                out1  += self.a[i][0]*F.conv2d(x[0], self.qB[i], self.bias, self.stride, self.padding, self.dilation, self.groups)\n",
    "\n",
    "            #x2 = self.input_quantizer2(input) # --> Input quantization\n",
    "            out2 = 0.0;\n",
    "            for i in range(self.M):\n",
    "                out2  += self.a[i][0]*F.conv2d(x[1], self.qB[i], self.bias, self.stride, self.padding, self.dilation, self.groups)\n",
    "\n",
    "            #x3 = self.input_quantizer3(input) # --> Input quantization\n",
    "            out3 = 0.0;\n",
    "            for i in range(self.M):\n",
    "                out3  += self.a[i][0]*F.conv2d(x[2], self.qB[i], self.bias, self.stride, self.padding, self.dilation, self.groups)\n",
    "\n",
    "            out = self.b[0][0]*out1 + self.b[1][0]*out2 + self.b[2][0]*out3 \n",
    "            \n",
    "            \n",
    "        elif self.N == 5 :\n",
    "            \n",
    "            x = self.input_quantizer(input) # --> Input quantization\n",
    "            self.a.data, self.qB.data = self.quantizer.quantize(self.weight)\n",
    "\n",
    "            out1 = 0.0;\n",
    "            for i in range(self.M):\n",
    "                out1  += self.a[i][0]*F.conv2d(x[0], self.qB[i], self.bias, self.stride, self.padding, self.dilation, self.groups)\n",
    "\n",
    "            #x2 = self.input_quantizer2(input) # --> Input quantization\n",
    "            out2 = 0.0;\n",
    "            for i in range(self.M):\n",
    "                out2  += self.a[i][0]*F.conv2d(x[1], self.qB[i], self.bias, self.stride, self.padding, self.dilation, self.groups)\n",
    "\n",
    "            #x3 = self.input_quantizer3(input) # --> Input quantization\n",
    "            out3 = 0.0;\n",
    "            for i in range(self.M):\n",
    "                out3  += self.a[i][0]*F.conv2d(x[2], self.qB[i], self.bias, self.stride, self.padding, self.dilation, self.groups)\n",
    "                \n",
    "            #x4 = self.input_quantizer4(input) # --> Input quantization\n",
    "            out4 = 0.0;\n",
    "            for i in range(self.M):\n",
    "                out4  += self.a[i][0]*F.conv2d(x[3], self.qB[i], self.bias, self.stride, self.padding, self.dilation, self.groups)\n",
    "\n",
    "            #x5 = self.input_quantizer5(input) # --> Input quantization\n",
    "            out5 = 0.0;\n",
    "            for i in range(self.M):\n",
    "                out5  += self.a[i][0]*F.conv2d(x[4], self.qB[i], self.bias, self.stride, self.padding, self.dilation, self.groups)\n",
    "\n",
    "            out = self.b[0][0]*out1 + self.b[1][0]*out2 + self.b[2][0]*out3 + self.b[3][0]*out4 + self.b[4][0]*out5\n",
    "        \n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class PReLU(Function):    \n",
    "    @staticmethod\n",
    "    def forward(ctx, tensor, gamma, eta, beta): \n",
    "        x = tensor-gamma\n",
    "        ctx.save_for_backward(x, beta)\n",
    "        \n",
    "        y = x.clone() #-->.clone() is necessary, otherwise on changing y, x also changes and we don't want that.\n",
    "        z = x.clone()\n",
    "        y[y<=0]=0\n",
    "        z[z>0]=0\n",
    "        \n",
    "        #z = torch.tensor([0.0]).cuda()\n",
    "        #y = torch.max(x,z)[0] + beta*torch.min(x,z)[0]\n",
    "        y = y + beta*z\n",
    "        y = y + eta\n",
    "        return y\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):        \n",
    "        # We return as many input gradients as there were arguments.\n",
    "        # Gradients of non-Tensor arguments to forward must be None.\n",
    "        g_o = grad_output.clone()\n",
    "        t,b = ctx.saved_tensors                                  \n",
    "        x = t.clone() #-->.clone() is necessary, otherwise on changing t, x also changes and we don't want that.\n",
    "        t[t<=0.0] = -1.0\n",
    "        t[t>0.0] = 0.0 \n",
    "        t = -1.0*t\n",
    "        grad_b_i = g_o*(x*t)\n",
    "        grad_g_i = g_o*(-t*b - (1.0 - t))  \n",
    "                                  \n",
    "        grad_input=g_o*(t*b + (1.0 - t))\n",
    "                                  \n",
    "        grad_g = (1/t.size()[0])*(torch.sum(torch.sum(torch.sum(grad_g_i,dim=0),dim=1),dim=1))\n",
    "        grad_gamma = grad_g.unsqueeze(0).unsqueeze(2).unsqueeze(2)\n",
    "                                  \n",
    "        grad_e = (1/t.size()[0])*(torch.sum(torch.sum(torch.sum(g_o,dim=0),dim=1),dim=1))\n",
    "        grad_eta = grad_e.unsqueeze(0).unsqueeze(2).unsqueeze(2)\n",
    "                                  \n",
    "        grad_b = (1/t.size()[0])*(torch.sum(torch.sum(torch.sum(grad_b_i,dim=0),dim=1),dim=1))\n",
    "        grad_beta = grad_b.unsqueeze(0).unsqueeze(2).unsqueeze(2)\n",
    "        return grad_input , grad_gamma, grad_eta, grad_beta\n",
    "                                                                            \n",
    "#-----------------------> (added on 17-08-2020)\n",
    "class PReLU_ActQuantizer(nn.ReLU) :\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(PReLU_ActQuantizer, self).__init__()\n",
    "        gamma = kwargs['gamma'] #this is a 1XC torch array.\n",
    "        gamma = gamma.unsqueeze(0).unsqueeze(2).unsqueeze(2)\n",
    "        self.gamma = nn.Parameter(gamma.float()) #initial clip_v value\n",
    "                                  \n",
    "        eta = kwargs['eta'] #this is a 1XC torch array.\n",
    "        eta = eta.unsqueeze(0).unsqueeze(2).unsqueeze(2)\n",
    "        self.eta = nn.Parameter(eta.float()) #initial clip_v value\n",
    "                                  \n",
    "        beta = kwargs['beta'] #this is a 1XC torch array.\n",
    "        beta = beta.unsqueeze(0).unsqueeze(2).unsqueeze(2)\n",
    "        self.beta = nn.Parameter(beta.float()) #initial clip_v value                          \n",
    "    \n",
    "    def forward(self, input):\n",
    "        x = PReLU.apply(input, self.gamma, self.eta, self.beta)#-->new addition\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def update_model_grads(net):\n",
    "    for n,m in net.named_modules():\n",
    "        if isinstance(m, QConv2d) :#or isinstance(m, QLinear):\n",
    "            m.update_grads()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 : Full precision weights and Input Activation Quantized\n",
    "###### the network has to be loaded with the most recent checkpoint and then the following transformation is performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crit = nn.NLLLoss(weight=class_weights, ignore_index=-1)\n",
    "net = DeepLabV3(num_classes=7)\n",
    "\n",
    "#starting from full precision networks and doing quantization for the first time. \n",
    "\n",
    "#net.load_state_dict(torch.load(\"/home/kaustavb/6867/weight/deeplabv3_resnet34_weights.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_channels = -1 #--->\n",
    "for n,m in net.named_modules():\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        n_channels = m.weight.size()[0] #---->\n",
    "        if n=='encoder.conv1' or n=='encoder.layer1.0.conv1' or n=='encoder.layer1.0.conv2' or n=='encoder.layer1.0.conv3' or n=='encoder.layer1.0.downsample.0' or n=='last_conv.8' : \n",
    "            #the first convlution layer remains as full-precision (first layer of ResNet-18 encoder) #it is called backbone.conv1 in DeepLabv3+ code.\n",
    "            continue\n",
    "        else :\n",
    "            #layer_id = int(n.partition('.')[-1].partition('.')[0]) #090719, AB: layer number for the conv layer\n",
    "            bias = False\n",
    "            if m.bias is not None:\n",
    "                bias = True\n",
    "            init_args = {'weight_data': m.weight.data,'bias_data': m.bias.data if bias else None, 'M':5, 'N':0} #added the 'alpha' variable which will be initialized from previously learned values.\n",
    "\n",
    "            #init_args = {'weight_data': m.weight.data,'bias_data': m.bias.data if bias else None, 'M':1, 'alpha_data':m.input_quantizer.alpha.data} #added the 'alpha' variable which will be initialized from previously learned values.\n",
    "\n",
    "            conv_args = {'in_channels': m.in_channels, 'out_channels': m.out_channels, 'kernel_size': m.kernel_size, 'stride': m.stride, 'padding': m.padding, 'groups': m.groups, 'bias': bias, 'dilation': m.dilation}\n",
    "            # ..............................................(14-10-2020)\n",
    "            #quant_args = {'B': 8}\n",
    "\n",
    "            #handling regular conv layers\n",
    "            #if n=='f2':\n",
    "            #conv = QConv2d_fpw(init_args = init_args, **conv_args) #final layer output is still floating point value.\n",
    "            #else:\n",
    "            conv = QConv2d(init_args = init_args, **conv_args)\n",
    "            rsetattr(net,n, conv)\n",
    "            print('CONV layer '+ n+ ' quantized using '+ 'ABC-Net method')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.load_state_dict(torch.load(\"/home/kaustavb/6867/weight/deeplabv3_resnet34_weights.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_channels = -1 #--->\n",
    "for n,m in net.named_modules():\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        n_channels = m.weight.size()[0] #---->\n",
    "        if n=='encoder.conv1' or n=='encoder.layer1.0.conv1' or n=='encoder.layer1.0.conv2' or n=='encoder.layer1.0.conv3' or n=='encoder.layer1.0.downsample.0' or n=='last_conv.8' : \n",
    "            #the first convlution layer remains as full-precision (first layer of ResNet-18 encoder) #it is called backbone.conv1 in DeepLabv3+ code.\n",
    "            continue\n",
    "        else :\n",
    "            #layer_id = int(n.partition('.')[-1].partition('.')[0]) #090719, AB: layer number for the conv layer\n",
    "            bias = False\n",
    "            if m.bias is not None:\n",
    "                bias = True\n",
    "            init_args = {'weight_data': m.weight.data,'bias_data': m.bias.data if bias else None, 'M':5, 'N':5} #added the 'alpha' variable which will be initialized from previously learned values.\n",
    "\n",
    "            #init_args = {'weight_data': m.weight.data,'bias_data': m.bias.data if bias else None, 'M':1, 'alpha_data':m.input_quantizer.alpha.data} #added the 'alpha' variable which will be initialized from previously learned values.\n",
    "\n",
    "            conv_args = {'in_channels': m.in_channels, 'out_channels': m.out_channels, 'kernel_size': m.kernel_size, 'stride': m.stride, 'padding': m.padding, 'groups': m.groups, 'bias': bias, 'dilation': m.dilation}\n",
    "            # ..............................................(14-10-2020)\n",
    "            #quant_args = {'B': 8}\n",
    "\n",
    "            #handling regular conv layers\n",
    "            #if n=='f2':\n",
    "            #conv = QConv2d_fpw(init_args = init_args, **conv_args) #final layer output is still floating point value.\n",
    "            #else:\n",
    "            conv = QConv2d(init_args = init_args, **conv_args)\n",
    "            rsetattr(net,n, conv)\n",
    "            print('CONV layer '+ n+ ' quantized using '+ 'ABC-Net method')\n",
    "    '''        \n",
    "    elif isinstance(m, nn.ReLU):#---->\n",
    "            i_qargs = {'gamma' : torch.randn(n_channels,), 'eta' : torch.randn(n_channels,), 'beta' : 0.25*torch.ones(n_channels,)}\n",
    "            relu = PReLU_ActQuantizer(**i_qargs)\n",
    "            rsetattr(net,n, relu)\n",
    "            print('RELU layer '+ n+ ' replaced using '+ 'ReAct-Net method')\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#segmentation_module = SegmentationModule(net, crit, n_class=7)\n",
    "#segmentation_module.cuda()\n",
    "\n",
    "#segmentation_module = torch.nn.DataParallel(segmentation_module, list(range(2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_quantize(segmentation_module, iterator,  optimizer_e, optimizer_d, history, epoch, len_iterator, n_epochs, \n",
    "          running_lr_e, running_lr_d, crit, n_class=20):\n",
    "    #---> len_iterator : one epoch has how many mini-batches (to traverse through the dataset)\n",
    "    #---> n_epochs : total number of epochs \n",
    "\n",
    "    ave_total_loss = AverageMeter()\n",
    "    ave_acc = AverageMeter()\n",
    "    ave_miou = AverageMeter()\n",
    "    \n",
    "    #segmentation_module.net.train()\n",
    "    #segmentation_module.zero_grad()\n",
    "    segmentation_module.train()\n",
    "    segmentation_module.zero_grad()\n",
    "    \n",
    "    # main loop\n",
    "    for i in range(len_iterator):\n",
    "        # load a batch of data\n",
    "        inputs, masks = next(iterator)\n",
    "        inputs, masks = inputs.cuda(), masks.cuda()\n",
    "        optimizer_e.zero_grad()\n",
    "        optimizer_d.zero_grad()\n",
    "\n",
    "        # adjust learning rate\n",
    "        #cur_iter = i + (epoch - 1) * len_iterator\n",
    "        #running_lr_encoder, running_lr_decoder = adjust_learning_rate(optimizers, cur_iter, len_iterator*n_epochs)\n",
    "        \n",
    "\n",
    "        # forward pass\n",
    "        #loss, acc, mIOU = segmentation_module(inputs.float(),masks.long())\n",
    "        \n",
    "        pred = segmentation_module(inputs.float()) #,masks.long())\n",
    "        loss = crit(pred, masks.long())\n",
    "        acc, mIOU = pixel_acc(pred, masks.long(), n_class)\n",
    "        \n",
    "        #print(loss)\n",
    "        #print(acc)\n",
    "        loss = loss.mean()\n",
    "        acc = acc.mean()\n",
    "        mIOU = mIOU.mean()\n",
    "        \n",
    "        #print(loss)\n",
    "        #print(acc)\n",
    "        \n",
    "        # Backward\n",
    "        loss.backward()\n",
    "        update_model_grads(segmentation_module) #----> new addition to update the parameters of quantized neural network\n",
    "        #for optimizer in optimizers:\n",
    "        optimizer_e.step()\n",
    "        optimizer_d.step()\n",
    "\n",
    "        # update average loss and acc\n",
    "        ave_total_loss.update(loss.data.item())\n",
    "        ave_acc.update(acc.data.item()*100)\n",
    "        ave_miou.update(mIOU.data.item()*100)\n",
    "\n",
    "        # calculate accuracy, and display\n",
    "        if i % 100 == 0:\n",
    "            print('Epoch: [{}][{}/{}], '\n",
    "                  'lr_encoder: {:.6f}, lr_decoder: {:.6f}, '\n",
    "                  'Accuracy: {:4.2f}, Loss: {:.6f}, mIOU: {:.6f}'\n",
    "                  .format(epoch, i, len_iterator,\n",
    "                          running_lr_e, running_lr_d,\n",
    "                          ave_acc.average(), ave_total_loss.average(), ave_miou.average()))\n",
    "\n",
    "            fractional_epoch = epoch - 1 + 1. * i / len_iterator\n",
    "            history['train']['epoch'].append(fractional_epoch)\n",
    "            history['train']['loss'].append(loss.data.item())\n",
    "            history['train']['acc'].append(acc.data.item())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#iterator_train = iter(dataloaders['train'])\n",
    "history = {'train': {'epoch': [], 'loss': [], 'acc': [], 'mIOU': []}, 'val': {'loss': [], 'acc': [], 'mIOU': []}}\n",
    "\n",
    "optimizers_enc, optimizers_dec = create_optimizers(net)\n",
    "\n",
    "net.cuda()\n",
    "net = torch.nn.DataParallel(net, list(range(1)))\n",
    "\n",
    "num_epoch=100\n",
    "\n",
    "best_acc = 0\n",
    "num_classes=7\n",
    "\n",
    "for epoch in range(num_epoch):\n",
    "    iterator_train = iter(dataloaders['train'])\n",
    "    running_lr_e, running_lr_d = adjust_learning_rate(optimizers_enc, optimizers_dec, epoch, num_epoch, \n",
    "                                                      lr_e=0.025, lr_d=0.050, lr_type='cos') \n",
    "    #lr changed once per epoch \n",
    "    train_quantize(net, iterator_train, optimizers_enc, optimizers_dec, history, epoch+1, len(dataloaders['train']), \n",
    "          num_epoch, running_lr_e, running_lr_d, crit, num_classes)\n",
    "    \n",
    "    iterator_val = iter(dataloaders['val'])\n",
    "    val(net, iterator_val, history, len(dataloaders['val']), crit, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_o = torch.from_numpy(np.array([-0.8,0,0.8])).unsqueeze(1).unsqueeze(1).unsqueeze(1).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_a = (1/g_o.size()[1])*(torch.sum(torch.sum(torch.sum(torch.sum(g_o,dim=1),dim=1),dim=1),dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_tensor = torch.randn(3,6,5,5)\n",
    "N=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_tensor = torch.cat(N*[torch.unsqueeze(orig_tensor,0)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_tensor[abs(new_tensor)>=1.0]=1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(5,).float()\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
